# UX Bench User Guide

This guide covers the end-to-end workflow for measuring and comparing interface efficiency using UX Bench.

---

## Part 1: Installation & Setup

### 1. Build the Tools
From the repository root, run:
```bash
make all
```
This produces:
-   **Recorder:** Unpacked extension in `recorder/dist/`
-   **Analyzer:** Binary executable at `cli/uxbench`

### 2. Install the Chrome Extension
1.  Open Chrome and navigate to `chrome://extensions`.
2.  Enable **Developer mode** (toggle in top-right).
3.  Click **Load unpacked**.
4.  Select the `recorder/dist` folder generated by the build.
5.  *Optional:* Pin the UX Bench icon to your toolbar for easy access.

### 3. Install the Analyzer CLI
To use `uxbench` globally in your terminal:
```bash
make install
```
*Note: Ensure your Go bin directory (typically `$HOME/go/bin`) is in your system `$PATH`.*

---

## Part 2: Using the Recorder

The Recorder is a "silent observer." It captures your interactions to calculate efficiency cost.

### Recording a Session
The Recorder now resides in the Chrome Side Panel for real-time telemetry.

1.  **Open the Side Panel:** Click the UX Bench icon or standard Chrome Side Panel icon.
2.  **Calibrate Viewport:**
    -   Select a resolution from the **VIEWPORT SIZE** dropdown (e.g., `1280x800`).
    -   *Crucial:* The browser window will automatically resize. Do not manually resize it afterwards.
    -   **START** button is disabled until a viewport is selected.
3.  **Start Recording:**
    -   Click **START** in the Side Panel.
    -   *Or* use the hotkey: `Cmd+Shift+R` (Mac) / `Ctrl+Shift+R` (Windows).
    -   Verify the status changes to `RECORDING` and the border turns green/red.
4.  **Perform the Task:**
    -   Complete your workflow.
    -   Observe **Live Telemetry** (Clicks, Depth, Scroll, Switches) updating in real-time.
5.  **Stop Recording:**
    -   Click **STOP** or press the hotkey.
    -   The run is added to the session buffer (`RUNS: 1`).

### Averaging Multiple Runs
To account for variance in human motor performance, you can record multiple runs of the *same* task.

1.  After stopping the first run, click **REPEAT TEST** (formerly Start).
2.  Reset the web page state (e.g., go back to start URL).
3.  Perform the task again and Stop.
4.  The counter will show `RUNS: 2`.
5.  **Download:** Click **DOWNLOAD**. This generates a single JSON file (e.g., `_AVG_2runs.json`) containing the **average** scores across all runs.
6.  **Clear:** Use **CLEAR** to discard current session data and start a fresh benchmark.

### Best Practices for Consistent Data
-   **One Task, One File:** Don't mix multiple distinct workflows in one recording.
-   **Reset State:** if comparing two apps, ensure both start from the equivalent "blank slate."
-   **No Extensions:** Use a clean Chrome profile or Incognito mode to prevent other extensions (ad blockers, password managers) from polluting the click stream.

---

## Part 3: Using the Analyzer

The Analyzer compares your recording files to reveal which interface demands less work.

### Basic Comparison
Run the CLI pointing to two or more JSON files:
```bash
uxbench compare old-design.json new-design.json
```
This launches the **Interactive TUI**.

### Navigating the TUI

| Key | Action |
|---|---|
| `↑` `↓` | **Navigate** through metrics rows |
| `Enter` | **Drill Down** to see *why* a metric is high (Diagnostic View) |
| `Esc` | **Back** to the previous view |
| `r` | **Radar View** – See the "shape" of efficiency tradeoffs |
| `s` | **Save Report** – Exports a markdown summary to `comparison_report.md` |
| `q` | **Quit** |

### Drill-Down Diagnostics
When your score is lower than the competitor's, press `Enter` on the losing metric to find out why.
-   **Fitts:** Tells you exactly which buttons were hardest to reach.
-   **Clicks:** Lists specific "Ceremonial" clicks (popups, toasts) you can remove.
-   **Scanning:** Identifies large visual jumps between related controls.

### Non-Interactive Reports
For sharing on GitHub or Slack without using the TUI:
```bash
# Generate a Markdown table with insights
uxbench compare --format markdown design_a.json design_b.json > results.md
```

---

## Troubleshooting

**"Extension Error: Service worker inactive"**
Manifest V3 service workers go to sleep. Open the Side Panel before recording -- it keeps the worker alive via telemetry polling. If the extension seems unresponsive, click the icon to wake it up.

**"CLI: command not found"**
Ensure `$HOME/go/bin` is in your shell `PATH`, or run the binary locally using `./cli/uxbench`.

**"Different Metrics Logic?"**
If comparing a Human recording vs a Playwright automation, some metrics (Decision Time, Mouse Hesitation) will be null for the bot. The Analyzer handles this gracefully but warns you.
